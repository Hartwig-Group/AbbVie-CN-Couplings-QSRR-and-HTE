{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"data/ligand-qsar/raw/alkylamine-ligand-modeling-unprocessed.tsv\", sep=\"\\t\")\n",
    "df = df[df[\"buchwald-type\"] > 0]\n",
    "\n",
    "# Define bins and labels\n",
    "bins = [-np.inf, 15, np.inf]\n",
    "labels = [0, 1]\n",
    "transformer = preprocessing.FunctionTransformer(pd.cut, kw_args={\"bins\": bins, \"labels\": labels, \"retbins\": False})\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = df.drop(columns=[\"ligand_1_name\", \"product_1_yield\", \"buchwald-type\"])\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Transform target\n",
    "y = df[\"product_1_yield\"]\n",
    "y_bin = transformer.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calcDrop(res: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"Calculate which columns to drop based on correlation matrix.\"\"\"\n",
    "    # All variables with correlation > cutoff\n",
    "    all_corr_vars = list(set(res[\"v1\"].tolist() + res[\"v2\"].tolist()))\n",
    "\n",
    "    # All unique variables in drop column\n",
    "    poss_drop = list(set(res[\"drop\"].tolist()))\n",
    "\n",
    "    # Keep any variable not in drop column\n",
    "    keep = list(set(all_corr_vars).difference(set(poss_drop)))\n",
    "\n",
    "    # Drop any variables in same row as a keep variable\n",
    "    p = res[res[\"v1\"].isin(keep) | res[\"v2\"].isin(keep)][[\"v1\", \"v2\"]]\n",
    "    q = list(set(p[\"v1\"].tolist() + p[\"v2\"].tolist()))\n",
    "    drop = list(set(q).difference(set(keep)))\n",
    "\n",
    "    # Remove drop variables from possible drop\n",
    "    poss_drop = list(set(poss_drop).difference(set(drop)))\n",
    "\n",
    "    # subset res dataframe to include possible drop pairs\n",
    "    m = res[res[\"v1\"].isin(poss_drop) | res[\"v2\"].isin(poss_drop)][[\"v1\", \"v2\", \"drop\"]]\n",
    "\n",
    "    # remove rows that are decided (drop), take set and add to drops\n",
    "    more_drop = set(list(m[~m[\"v1\"].isin(drop) & ~m[\"v2\"].isin(drop)][\"drop\"]))\n",
    "    for item in more_drop:\n",
    "        drop.append(item)\n",
    "\n",
    "    return drop\n",
    "\n",
    "\n",
    "def corrX_new(df: pd.DataFrame, cut: float = 0.9):\n",
    "    \"\"\"Calculate the correlation matrix and return the columns to drop.\"\"\"\n",
    "    corr_mtx = df.corr().abs()\n",
    "    avg_corr = corr_mtx.mean(axis=1)\n",
    "    up = corr_mtx.where(np.triu(np.ones(corr_mtx.shape), k=1).astype(bool))\n",
    "\n",
    "    dropcols = []\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for row in range(len(up) - 1):\n",
    "        col_idx = row + 1\n",
    "        for col in range(col_idx, len(up)):\n",
    "            if corr_mtx.iloc[row, col] > cut:\n",
    "                if avg_corr.iloc[row] > avg_corr.iloc[col]:\n",
    "                    dropcols.append(row)\n",
    "                    drop = corr_mtx.columns[row]\n",
    "                else:\n",
    "                    dropcols.append(col)\n",
    "                    drop = corr_mtx.columns[col]\n",
    "\n",
    "                s = [\n",
    "                    corr_mtx.index[row],\n",
    "                    up.columns[col],\n",
    "                    avg_corr.iloc[row],\n",
    "                    avg_corr.iloc[col],\n",
    "                    up.iloc[row, col],\n",
    "                    drop,\n",
    "                ]\n",
    "\n",
    "                res.append(s)\n",
    "\n",
    "    res = pd.DataFrame(res, columns=([\"v1\", \"v2\", \"v1.target\", \"v2.target\", \"corr\", \"drop\"]))\n",
    "\n",
    "    dropcols_names = calcDrop(res)\n",
    "\n",
    "    return dropcols_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P_int_phosphine', 'buried_volume_3.5A', 'distance_Pd_N', 'distance_carb_OH_O', 'fukui_f_minus_carb_o', 'fukui_f_minus_ipso_carbon', 'fukui_f_plus_amine_nitrogen', 'fukui_f_plus_aryl_carbon', 'fukui_f_plus_avg_amine_proton', 'fukui_f_zero_metal', 'max_buried_volume_5.0A', 'max_buried_volume_ipso_3.5A', 'max_distance_carb_OH_O', 'max_fukui_f_minus_avg_amine_proton', 'max_fukui_f_minus_carb_o', 'max_fukui_f_minus_metal', 'max_fukui_f_plus_ipso_carbon', 'max_fukui_f_plus_metal', 'max_fukui_f_zero_avg_amine_proton', 'max_partial_charge_amine_carbon', 'max_partial_charge_amine_nitrogen', 'max_partial_charge_carbon', 'max_quadrant_buried_volume_ligand_max', 'max_quadrant_total_volume_ligand_range_max', 'min_buried_volume_5.0A', 'min_distance_carb_OH_O', 'min_fukui_f_minus_amine_nitrogen', 'min_fukui_f_minus_carb_o', 'min_fukui_f_plus_carb_o', 'min_fukui_f_zero_carb_oh', 'min_fukui_f_zero_ipso_carbon', 'min_global_nucleophilicity', 'min_partial_charge_carboxylic_oxygen', 'min_partial_charge_ipso_carbon', 'min_quadrant_buried_volume_ligand_max', 'partial_charge_carboxylic_oxygen', 'partial_charge_phosphine', 'pyramidalization_P', 'sterimol_ligand_L']\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "run = True\n",
    "\n",
    "if run:\n",
    "    drop_cols = corrX_new(X, cut=0.90)\n",
    "\n",
    "    X = X.drop(columns=drop_cols)\n",
    "\n",
    "    print(sorted(drop_cols))\n",
    "    print(len(drop_cols))\n",
    "\n",
    "    df.drop(columns=drop_cols).to_csv(\"data/ligand-qsar/alkylamine-ligand-modeling-full.tsv\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd8015c13ed42088a82a230cd6801ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BoostaGRoota round:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['buried_volume_2.0A', 'P_int_ipso_carbon',\n",
      "       'min_distance_avg_amine_N_H', 'min_partial_charge_phosphine',\n",
      "       'max_buried_volume_3.5A', 'max_P_int_ligand',\n",
      "       'max_sasa_ligand_area', 'max_sterimol_ligand_L',\n",
      "       'max_partial_charge_phosphine', 'max_fukui_f_zero_ipso_carbon'],\n",
      "      dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "import arfs.feature_selection.allrelevant as arfsgroot\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "selected_features = []\n",
    "\n",
    "seed = 1\n",
    "model = LGBMClassifier(random_state=seed, verbose=-1, class_weight=\"balanced\")\n",
    "\n",
    "feat_selector = arfsgroot.BoostAGroota(estimator=model, importance=\"shap\")\n",
    "feat_selector.fit(X, y_bin)\n",
    "\n",
    "selected_features.append(feat_selector.selected_features_)\n",
    "\n",
    "# feat_selector.plot_importance(n_feat_per_inch=3)\n",
    "# plt.show()\n",
    "print(list(selected_features))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
